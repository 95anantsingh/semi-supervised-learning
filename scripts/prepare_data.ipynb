{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from datasets import logging as dlog\n",
    "from transformers import logging as tlog\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import Features, Dataset\n",
    "from datasets.load import load_dataset, load_dataset_builder, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cm001.hpc.nyu.edu\n"
     ]
    }
   ],
   "source": [
    "global_seed = 100\n",
    "random.seed(global_seed)\n",
    "tlog.set_verbosity_error()\n",
    "dlog.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!hostname\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/scratch/as14229/envs_dirs/NLP/lib/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBPedia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa06ad33b6b45ff9499ba337aa457ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'dbpedia_14'\n",
    "cache_dir = './../.cache'\n",
    "data_path= './../data'\n",
    "\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir, save_infos=True)\n",
    "db_info = load_dataset_builder(dataset_name).info\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'yahoo_answers_10'\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "csv_dir = os.path.join(data_dir,'csv')\n",
    "\n",
    "if not os.path.exists(csv_dir):\n",
    "    data_url = \"https://docs.google.com/uc?id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU&amp\\\n",
    "        ;export=download&confirm=t&uuid=f25e7f13-9597-4e0b-9061-c9fe340eaa8e\"\n",
    "    tar_file = os.path.join(data_dir,'yahoo_answers_csv.tar.gz')\n",
    "    # download\n",
    "    with open(tar_file,'wb') as file:\n",
    "        file.write(requests.get(data_url,allow_redirects=True).content)\n",
    "    # extract\n",
    "    with tarfile.open(tar_file) as file:\n",
    "        file.extractall(data_dir)\n",
    "    # delete\n",
    "    os.remove(tar_file)\n",
    "    # rename\n",
    "    os.rename(os.path.join(data_dir,'yahoo_answers_csv'),csv_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_header = ['label', 'question_title', 'question_content', 'content']\n",
    "classes = ['Society & Culture', 'Science & Mathematics', 'Health', \\\n",
    "    'Education & Reference', 'Computers & Internet', 'Sports', 'Business & Finance', 'Entertainment & Music',\\\n",
    "    'Family & Relationships', 'Politics & Government']\n",
    "feature_dict = {'label': {'names': classes, '_type':'ClassLabel', 'id':None},\n",
    "    'question_title': {'dtype': 'string', '_type':'Value', 'id':None},\n",
    "    'question_content': {'dtype': 'string', '_type':'Value', 'id':None},\n",
    "    'content': {'dtype': 'string', '_type':'Value', 'id':None}\n",
    "    }\n",
    "features = Features.from_dict(feature_dict)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(csv_dir,'train.csv'),names=data_header).dropna(axis=0,subset=['content']).reset_index(drop=True)\n",
    "test_df = pd.read_csv(os.path.join(csv_dir,'test.csv'),names=data_header).dropna(axis=0,subset=['content']).reset_index(drop=True)\n",
    "\n",
    "train_df.label -= 1\n",
    "test_df.label -= 1\n",
    "\n",
    "train_set = Dataset.from_pandas(train_df,features=features,cache_dir=data_dir)\n",
    "test_set = Dataset.from_pandas(test_df,features=features,cache_dir=data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.sort(column='label', keep_in_memory=True)\n",
    "test_set=test_set.sort(column='label', keep_in_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=128\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=\"max_length\", max_length=MAX_LENGTH ,truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train_set = tokenized_train_set.rename_column(\"label\",\"labels\")\n",
    "tokenized_train_set = tokenized_train_set.with_format(\"torch\")\n",
    "\n",
    "tokenized_test_set = tokenized_test_set.rename_column(\"label\",\"labels\")\n",
    "tokenized_test_set = tokenized_test_set.with_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_set.save_to_disk(data_dir+'/tokenized/train')\n",
    "tokenized_test_set.save_to_disk(data_dir+'/tokenized/test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_set = load_from_disk(data_dir+'/tokenized/train')\n",
    "tokenized_test_set  = load_from_disk(data_dir+'/tokenized/test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Society & Culture', 'Science & Mathematics', 'Health', 'Education & Reference', 'Computers & Internet', 'Sports', 'Business & Finance', 'Entertainment & Music', 'Family & Relationships', 'Politics & Government']\n",
      "\n",
      "{'Society & Culture': 0, 'Science & Mathematics': 1, 'Health': 2, 'Education & Reference': 3, 'Computers & Internet': 4, 'Sports': 5, 'Business & Finance': 6, 'Entertainment & Music': 7, 'Family & Relationships': 8, 'Politics & Government': 9}\n",
      "\n",
      "Dataset({\n",
      "    features: ['labels', 'question_title', 'question_content', 'content', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1375421\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'question_title', 'question_content', 'content', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 58966\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_set.features[\"labels\"].names) # All label names ` \n",
    "print()\n",
    "print(tokenized_train_set.features[\"labels\"]._str2int) # Mapping from labels to integer\n",
    "print()\n",
    "print(tokenized_train_set)\n",
    "print(tokenized_test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 138700, 1: 139991, 2: 136996, 3: 137633, 4: 134149, 5: 139890, 6: 137916, 7: 137577, 8: 133902, 9: 138667}\n",
      "{0: 5936, 1: 5999, 2: 5874, 3: 5910, 4: 5736, 5: 5996, 6: 5917, 7: 5897, 8: 5760, 9: 5941}\n"
     ]
    }
   ],
   "source": [
    "def get_class_distribution(data):\n",
    "    classes = [*range(tokenized_train_set.features[\"labels\"].num_classes)]\n",
    "    distribution = {clss:0 for clss in sorted(classes)}\n",
    "    for val in data['labels'].numpy():\n",
    "        distribution[val] += 1 \n",
    "    return distribution\n",
    "\n",
    "print(get_class_distribution(tokenized_train_set))\n",
    "print(get_class_distribution(tokenized_test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d16b7065a7b6c736c1e1086f6450fd457f50d264e4008e1f14cb5793d37f9a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
