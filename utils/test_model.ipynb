{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets.load import load_from_disk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, DataCollatorWithPadding\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDataset(Dataset):\n",
    "    def __init__(self, dataset, indices=None, labels=None) -> None:\n",
    "        self.indices = indices\n",
    "        self.data = dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.data[index]['input_ids']\n",
    "        attention_mask = self.data[index]['attention_mask']\n",
    "        label = self.labels[index] if self.labels != None else self.data[index]['labels']\n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'label':label}\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.indices: return len(self.indices)\n",
    "        else: return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "test_data  = load_from_disk('data/yahoo_answers/tokenized/test') \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "criterion = CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n",
    "            num_labels=10, \n",
    "            cache_dir='.cache/model')\n",
    "model.to(device)\n",
    "\n",
    "test_data = STDataset(test_data)\n",
    "test_dataloader = DataLoader(test_data, batch_size=768,collate_fn=data_collator)\n",
    "\n",
    "acc_metric = MulticlassAccuracy(device=device)\n",
    "cw_f1_metric = MulticlassF1Score(num_classes=10, average=None, device=device)\n",
    "cw_acc_metric = MulticlassAccuracy(num_classes=10, average=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(model, dataloader, progress_bar):\n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(input.to(device) for input in batch.data.values())\n",
    "        \n",
    "        input_ids, attension_mask = batch[0], batch[1]\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attension_mask)\n",
    "            labels= batch[2]\n",
    "            loss = criterion(output.logits, labels)\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "        acc_metric.update(output.logits, labels)\n",
    "        cw_f1_metric.update(output.logits, labels)\n",
    "        cw_acc_metric.update(output.logits, labels)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    \n",
    "    accuracy = acc_metric.compute().item()\n",
    "    cw_f1_score = (cw_f1_metric.compute().cpu().numpy()).tolist()\n",
    "    cw_accuracy = (cw_acc_metric.compute().cpu().numpy()).tolist()\n",
    "\n",
    "    acc_metric.reset()\n",
    "    cw_f1_metric.reset()\n",
    "    cw_acc_metric.reset()\n",
    "\n",
    "    return eval_loss, cw_f1_score, accuracy, cw_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation :   0%|\u001b[37m          \u001b[0m| 0/77 [00:00<?, ? batch/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: .cache/v3.0/data/yahoo_answers/eval_model.pt\n",
      "Test Loss: 1.0112 ; Test Acc: 66.8877\n",
      "\n",
      "\n",
      "CW Acc: [0.5116239786148071, 0.7444574236869812, 0.7701736688613892, 0.4702199697494507, 0.8294979333877563, 0.795530378818512, 0.4429609477519989, 0.6596574783325195, 0.7449652552604675, 0.7246254682540894]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation :   0%|\u001b[37m          \u001b[0m| 0/77 [00:00<?, ? batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_file,map_location\u001b[39m=\u001b[39mdevice))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m eval_pbar \u001b[39m=\u001b[39m tqdm(desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation \u001b[39m\u001b[39m'\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m batch\u001b[39m\u001b[39m'\u001b[39m, colour\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m, total\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test_dataloader))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ev_loss, _, ev_acc, ev_cw_acc \u001b[39m=\u001b[39m _eval(model, test_dataloader, eval_pbar)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m eval_pbar\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mFile: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTest Loss: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m ; Test Acc: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                                 \u001b[39m%\u001b[39m(model_file, ev_loss, ev_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m eval_pbar\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mCW Acc: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(ev_cw_acc))\n",
      "\u001b[1;32m/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb Cell 5\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(model, dataloader, progress_bar)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     labels\u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mlogits, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m eval_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m acc_metric\u001b[39m.\u001b[39mupdate(output\u001b[39m.\u001b[39mlogits, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bga015.hpc.nyu.edu/home/as14229/NYU_HPC/semi-supervised-learning/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m cw_f1_metric\u001b[39m.\u001b[39mupdate(output\u001b[39m.\u001b[39mlogits, labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_files = ['.cache/v3.0/data/yahoo_answers/eval_model.pt','.cache/model/yahoo_answers_test_model_org.pt',\n",
    "                '.cache/model/yahoo_answers_test_model_sch.pt','.cache/model/yahoo_answers_test_model_sch2.pt']\n",
    "for model_file in model_files:\n",
    "    model.load_state_dict(torch.load(model_file,map_location=device))\n",
    "    eval_pbar = tqdm(desc='Validation ', unit=' batch', colour='white', total= len(test_dataloader))\n",
    "    ev_loss, _, ev_acc, ev_cw_acc = _eval(model, test_dataloader, eval_pbar)\n",
    "    eval_pbar.write('File: %s\\nTest Loss: %.4f ; Test Acc: %.4f\\n\\n' \\\n",
    "                                    %(model_file, ev_loss, ev_acc*100))\n",
    "    eval_pbar.write('CW Acc: '+str(ev_cw_acc))\n",
    "    eval_pbar.reset()\n",
    "eval_pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d16b7065a7b6c736c1e1086f6450fd457f50d264e4008e1f14cb5793d37f9a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
