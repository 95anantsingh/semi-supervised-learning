{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from datasets import logging as dlog\n",
    "from transformers import logging as tlog\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import Features, Dataset\n",
    "from datasets.load import load_dataset, load_dataset_builder, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr024.hpc.nyu.edu\n"
     ]
    }
   ],
   "source": [
    "data_path = './../data'\n",
    "cache_path = './../.cache'\n",
    "\n",
    "global_seed = 100\n",
    "random.seed(global_seed)\n",
    "tlog.set_verbosity_error()\n",
    "dlog.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!hostname\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/scratch/as14229/envs_dirs/NLP/lib/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=128\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "content_key = 'content'\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[content_key], padding=\"max_length\", max_length=MAX_LENGTH ,truncation=True)\n",
    "\n",
    "def tokenize_dataset(data_dir,train_set,test_set,key='content'):\n",
    "    # Content key\n",
    "    global content_key\n",
    "    content_key = key\n",
    "\n",
    "    # Sort\n",
    "    train_set = train_set.sort(column='label')\n",
    "    test_set = test_set.sort(column='label')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "    tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Format\n",
    "    tokenized_train_set = tokenized_train_set.rename_column(\"label\",\"labels\")\n",
    "    tokenized_test_set = tokenized_test_set.rename_column(\"label\",\"labels\")\n",
    "    \n",
    "    if content_key!='content':\n",
    "        tokenized_train_set = tokenized_train_set.rename_column(content_key,\"content\")\n",
    "        tokenized_test_set = tokenized_test_set.rename_column(content_key,\"content\")\n",
    "    \n",
    "    tokenized_train_set = tokenized_train_set.with_format(\"torch\")\n",
    "    tokenized_test_set = tokenized_test_set.with_format(\"torch\")\n",
    "\n",
    "    # Save\n",
    "    tokenized_train_set.save_to_disk(data_dir+'/tokenized/train')\n",
    "    tokenized_test_set.save_to_disk(data_dir+'/tokenized/test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9de4d9ff147588aabd78131781a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'dbpedia_14'\n",
    "\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_path, save_infos=True)\n",
    "db_info = load_dataset_builder(dataset_name).info\n",
    "\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_dataset(data_dir,train_set,test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'yahoo_answers'\n",
    "\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "csv_dir = os.path.join(data_dir,'csv')\n",
    "\n",
    "if not os.path.exists(csv_dir):\n",
    "    data_url = \"https://docs.google.com/uc?id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU&amp\\\n",
    "        ;export=download&confirm=t&uuid=f25e7f13-9597-4e0b-9061-c9fe340eaa8e\"\n",
    "    tar_file = os.path.join(data_dir,'yahoo_answers_csv.tar.gz')\n",
    "    # download\n",
    "    with open(tar_file,'wb') as file:\n",
    "        file.write(requests.get(data_url,allow_redirects=True).content)\n",
    "    # extract\n",
    "    with tarfile.open(tar_file) as file:\n",
    "        file.extractall(data_dir)\n",
    "    # delete\n",
    "    os.remove(tar_file)\n",
    "    # rename\n",
    "    os.rename(os.path.join(data_dir,'yahoo_answers_csv'),csv_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_header = ['label', 'question_title', 'question_content', 'content']\n",
    "classes = ['Society & Culture', 'Science & Mathematics', 'Health', \\\n",
    "    'Education & Reference', 'Computers & Internet', 'Sports', 'Business & Finance', 'Entertainment & Music',\\\n",
    "    'Family & Relationships', 'Politics & Government']\n",
    "feature_dict = {'label': {'names': classes, '_type':'ClassLabel', 'id':None},\n",
    "    'question_title': {'dtype': 'string', '_type':'Value', 'id':None},\n",
    "    'question_content': {'dtype': 'string', '_type':'Value', 'id':None},\n",
    "    'content': {'dtype': 'string', '_type':'Value', 'id':None}\n",
    "    }\n",
    "features = Features.from_dict(feature_dict)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(csv_dir,'train.csv'),names=data_header).dropna(axis=0,subset=['content']).reset_index(drop=True)\n",
    "test_df = pd.read_csv(os.path.join(csv_dir,'test.csv'),names=data_header).dropna(axis=0,subset=['content']).reset_index(drop=True)\n",
    "\n",
    "train_df.label -= 1\n",
    "test_df.label -= 1\n",
    "\n",
    "train_set = Dataset.from_pandas(train_df,features=features)\n",
    "test_set = Dataset.from_pandas(test_df,features=features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_dataset(data_dir,train_set,test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AG News"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e64583f83894774ad8d4f6c78044b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'ag_news'\n",
    "\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_path, save_infos=True)\n",
    "db_info = load_dataset_builder(dataset_name).info\n",
    "\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2eba63453f4e0c88e82e20e5869ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1400ac8132bd4e13923d97eac52403a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_dataset(data_dir,train_set,test_set,key='text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdbde94b52840aa8e48cbc463fbd560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'yelp_review_full'\n",
    "\n",
    "data_dir = os.path.join(data_path, dataset_name)\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_path, save_infos=True)\n",
    "db_info = load_dataset_builder(dataset_name).info\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b723026592974cb2a8c8d3d0e17ab314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/650 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246663993a144e83931062cff496de25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_dataset(data_dir,train_set,test_set,key='text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'yahoo_answers'\n",
    "data_dir = os.path.join(data_path,dataset_name)\n",
    "\n",
    "tokenized_train_set = load_from_disk(data_dir+'/tokenized/train')\n",
    "tokenized_test_set  = load_from_disk(data_dir+'/tokenized/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'labels': ClassLabel(names=['Society & Culture', 'Science & Mathematics', 'Health', 'Education & Reference', 'Computers & Internet', 'Sports', 'Business & Finance', 'Entertainment & Music', 'Family & Relationships', 'Politics & Government'], id=None), 'question_title': Value(dtype='string', id=None), 'question_content': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_set.info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: yahoo_answers\n",
      "\n",
      " ['Society & Culture', 'Science & Mathematics', 'Health', 'Education & Reference', 'Computers & Internet', 'Sports', 'Business & Finance', 'Entertainment & Music', 'Family & Relationships', 'Politics & Government']\n",
      "\n",
      "{'Society & Culture': 0, 'Science & Mathematics': 1, 'Health': 2, 'Education & Reference': 3, 'Computers & Internet': 4, 'Sports': 5, 'Business & Finance': 6, 'Entertainment & Music': 7, 'Family & Relationships': 8, 'Politics & Government': 9}\n",
      "\n",
      "Dataset({\n",
      "    features: ['labels', 'question_title', 'question_content', 'content', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 58966\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'question_title', 'question_content', 'content', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1375421\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print('Dataset:',dataset_name)\n",
    "print('\\n',tokenized_train_set.features[\"labels\"].names) # All label names ` \n",
    "print()\n",
    "print(tokenized_train_set.features[\"labels\"]._str2int) # Mapping from labels to integer\n",
    "print()\n",
    "print(tokenized_test_set)\n",
    "print(tokenized_train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5936, 1: 5999, 2: 5874, 3: 5910, 4: 5736, 5: 5996, 6: 5917, 7: 5897, 8: 5760, 9: 5941}\n",
      "{0: 138700, 1: 139991, 2: 136996, 3: 137633, 4: 134149, 5: 139890, 6: 137916, 7: 137577, 8: 133902, 9: 138667}\n"
     ]
    }
   ],
   "source": [
    "def get_class_distribution(data):\n",
    "    classes = [*range(tokenized_train_set.features[\"labels\"].num_classes)]\n",
    "    distribution = {clss:0 for clss in sorted(classes)}\n",
    "    for val in data['labels'].numpy():\n",
    "        distribution[val] += 1 \n",
    "    return distribution\n",
    "\n",
    "print(get_class_distribution(tokenized_test_set))\n",
    "print(get_class_distribution(tokenized_train_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = {'input_ids', 'attention_mask'}\n",
    "\n",
    "data_dir = './../data'\n",
    "other_datasets = ['yelp_review_full','ag_news']       \n",
    "\n",
    "datasets = []\n",
    "for dataset_name in other_datasets:\n",
    "    dataset = load_from_disk(os.path.join(data_dir,dataset_name,'tokenized/train'))\n",
    "    labels = dataset['labels']\n",
    "    content = dataset['content']\n",
    "    dataset = dataset.remove_columns(list(set(dataset.features.keys())-columns_to_keep))\n",
    "    dataset = dataset.add_column('labels', labels.tolist())\n",
    "    dataset = dataset.add_column('content', content)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "other_data = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels', 'content']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770000 770000\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0].shape[0] + datasets[1].shape[0], other_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[1],[2],[3],[4]])\n",
    "preds = torch.tensor([1,3,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = (preds != targets.view_as(preds)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(os.path.join('./../data/','yahoo_answers','tokenized/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds != targets.view_as(preds)).logical_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds != targets.view_as(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'question_title', 'question_content', 'content', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(wrong_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0, 1]),\n",
       " 'question_title': ['Which race is considered the Majority now? Blacks? Hispanics? or Whites?',\n",
       "  'Try to solve this economical problem....and math also?'],\n",
       " 'question_content': [None,\n",
       "  'If we have corporation that produces some product and we are given informations on the size of costs depending of production level\\\\n\\\\nProduction (x)   1   ;   4   ;   5   ;   7   ; 10\\\\nCosts              2.5  ;  6.5 ;  7.8 ; 10.6 ; 15.6\\\\n\\\\n1) Define the function of costs if it has the form of\\\\n a x^2 + bx + c\\\\n\\\\n2) Find the production level for which we have the minimal average costs.\\\\n\\\\n3)  Define elasticity (E)  of total costs   and  elasticity of average costs \\\\n\\\\nTry, it will help me a lot'],\n",
       " 'content': ['According to the 2005 CIA World Fact (an official statistics agency), America racial composition is:\\\\n\\\\n    * White\\\\n          o 81.7%, or 241 million (includes those who declared themselves as white-Hispanics; those of Middle Eastern and North African descent; and others who checked \"Some other race\".)\\\\n          o 69%, or 204 million (excludes white-Hispanics, but includes Middle Easterners, North Africans, and others who checked \"Some other race\".)\\\\n    * Black or African American 12.9% or 36.4 million,\\\\n    * Asian 4.2% or 11.9 million,\\\\n    * American Indian 1.4% or 4.1 million,which includes those of mixed race or more than race in addition to Native Americans\\\\n    * Native Hawaiian or other Pacific Islander 0.2%\\\\n    * Two or more races 2.4%\\\\n\\\\nThe Census Bureau considers Hispanic to be any person with national origins in Latin America or Spain (ie. Spaniards, Cubans, Mexicans, Puerto Ricans, Dominicans, etc.), and thus may be of any race.\\\\n\\\\nThe American population is therefore only around 60% pure White European(180 million people).\\\\n\\\\n    * Hispanics of any race 14.1%, or 41.3 million.\\\\n\\\\nBy ethnicity, Hispanics comprise 14% of the American population, surpassing African Americans as America\\'s largest de facto ethnic minority.\\\\n\\\\nThe Census Bureau\\'s definition of \"white\" is not necessarily the definition most widely held by Americans in general. Most Americans define \"white\" to exclude all Hispanics, even those of exclusive or predominanty European descent. Using that definition, the white proportion of the U.S. population is currently at 69.1% or even much less if we take into account the important amount of them who define themselves as white being actually of mixed ancestry.\\\\n\\\\nOf course, by the same definition, the numbers for each of the other races would be reduced if one were to take into account the important amounts of each group who define themselve as mixed ancestry rather than solely African American, Asian American or Native American.',\n",
       "  \"I hope you appreciate this since it is a bit of work. Hopefully it will help you solve similar problems. There are problems with the data you presented but you will get the idea of how to solve these problems.\\\\n\\\\n1) the cost function C(x) is:\\\\n \\\\nC(x) = (-x^2+165x+136)/120\\\\n\\\\nYou need to determine a,b,c from 3 independent equations of the form ax^2+bx+c = specified value.You can choose any 3 of the 5 data points to determine a,b,c. I chose the first 3 and determined a,b,c. However, though the above function works for x =1,4,5 it does not work for x=7 or 10. So the data is inconsistent. Also, this is not a realistic cost function since if x>165.82 the cost becomes negative.\\\\n\\\\nIf x =1....  a+b+c=2.5\\\\nIf x=4...16a+4b+c=6.5\\\\nTaking the difference between these equations gives:\\\\n15a+3b=4 \\\\nb = (4-15a)/3 = (4/3)-5a\\\\n\\\\nIf x= 5....25a+5b+c = 7.8\\\\nor using the value of b above\\\\n25a+5(4/3 -5a)+c =7.8\\\\n25a+20/3-25a+c = 7.8\\\\nc=7.8-20/3 = 39/5 - 20/3 = (117-100)/15 = 17/15 = 136/120\\\\n\\\\na+b+c=2.5\\\\nSo a+4/3-5a+17/15 = 2.5\\\\n-4a = 2.5-17/15-4/3 = (37.5 -17-20)/15 = 0.5/15\\\\na= -0.5/60 = -1/120\\\\n\\\\nb=(4/3)-5a = 4/3-5(-1/120) = 4/3+5/120 = 165/120\\\\n\\\\n2) The average cost per unit x is:\\\\n\\\\n C(x)/x = -x/120 + 165/120 + 136/120x\\\\n\\\\nd/dx C(x)/x = -1/120 - 136/120x^2 \\\\n\\\\nWhen the average cost is a minimum this must = 0\\\\n\\\\nSo 0=1+136/x^2 but this would make x^2 = -136 which is impossible. It would still not make since if it was the Total cost which you were trying to minimize, snce, as mentioned, for large enough x this becomes negative and increasingly so.\\\\n\\\\n3) The total cost is given by the the function C(x). The elasicity E is the sensitivity of the total cost to a change in production.\\\\n\\\\nSo E = d C(x)/dx = C' (x) = (-2x+165)/120\\\\n\\\\nthe average cost per unit x is:\\\\n\\\\n C(x)/x = -x/120 + 165/120 + 136/120x\\\\n\\\\nThe elasticity is again given by the derivative of this funcion:\\\\n\\\\n= d/dx C(x)/x = -1/120 - 136/120x^2\\\\n= (-1/120)(1+136/x^2)\\\\n\\\\nWe see that this elasticity is negative.\"],\n",
       " 'input_ids': tensor([[    0, 14693,     7,     5,  4013,  8335,   623, 18454,    36,   260,\n",
       "            781,  6732,  1218,   238,   730,  6689, 15229,    16, 48347,   282,\n",
       "          37457,   282,  1437,  1437,  1437,  1009,   735, 37457,   282,  1437,\n",
       "           1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1021,  7694,\n",
       "              4,   406,  4234,    50, 35752,   153,    36, 25142,   167,    54,\n",
       "           2998,  1235,    25,  1104,    12,  9962, 12560,  2857,   131,   167,\n",
       "              9,  2367,  3877,     8,   369,  1704, 19276,   131,     8,   643,\n",
       "             54,  7869,    22,  6323,    97,  1015,   845, 49394,   282,  1437,\n",
       "           1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1021,  5913,\n",
       "           4234,    50, 28325,   153,    36,  3463, 44313,  1104,    12,  9962,\n",
       "          12560,  2857,     6,    53,  1171,  2367,  3877,   268,     6,   369,\n",
       "          16056,     6,     8,   643,    54,  7869,    22,  6323,    97,  1015,\n",
       "            845, 49394,   282,  1437,  1437,  1437,  1009,     2],\n",
       "         [    0,   100,  1034,    47,  5478,    42,   187,    24,    16,    10,\n",
       "            828,     9,   173,     4, 13088,    24,    40,   244,    47,  6136,\n",
       "           1122,  1272,     4,   345,    32,  1272,    19,     5,   414,    47,\n",
       "           2633,    53,    47,    40,   120,     5,  1114,     9,   141,     7,\n",
       "           6136,   209,  1272,     4, 37457,   282, 37457,   282,   134,    43,\n",
       "              5,   701,  5043,   230,  1640,  1178,    43,    16, 48347,   282,\n",
       "          44128,   282,   347,  1640,  1178,    43,  5457, 15611,  1178, 35227,\n",
       "            176,  2744, 21933,  1178,  2744, 25966, 36098, 10213, 37457,   282,\n",
       "          37457,   282,  1185,   240,     7,  3094,    10,     6,   428,     6,\n",
       "            438,    31,   155,  2222, 43123,     9,     5,  1026, 18884, 35227,\n",
       "            176,  2744,   428,  1178,  2744,   438,  5457, 17966,   923,     4,\n",
       "           1185,    64,  2807,   143,   155,     9,     5,   195,   414,   332,\n",
       "              7,  3094,    10,     6,   428,     6,   438,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select([150000,0]).sort(column='labels')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_samples_cw_idx = {0:[1,2], 1:[1,2], 2:[1,2]}\n",
    "wrong_samples_cw_idx = {0:[1,2], 1:[1,2], 2:[1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 0.5, 2: 0.5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{cr_key:len(cr_val)/(len(cr_val)+len(wr_val)) for (cr_key, cr_val),(_, wr_val) in zip(correct_samples_cw_idx.items(), wrong_samples_cw_idx.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./test.log' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('./test.log','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "torch.save(a,'./a.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.load('./a.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "INFO:__main__:console logging redirected to `tqdm.write()`\n",
      "100%|██████████| 9/9 [00:09<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    with logging_redirect_tqdm():\n",
    "        for i in trange(9):\n",
    "            sleep(1)\n",
    "            LOG.info(\"console logging redirected to `tqdm.write()`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d16b7065a7b6c736c1e1086f6450fd457f50d264e4008e1f14cb5793d37f9a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
